{"cells":[{"cell_type":"markdown","metadata":{"id":"AV4g7YkC25iC"},"source":["# Homework 8\n","\n","In this homework, you will train a multi-label text classifier on a subset of [AG News](https://huggingface.co/datasets/r-three/ag_news_subset) dataset using a pre-trained BERT model. The AG News dataset consists of news articles categorized into one of four topics (0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech).\n","\n","**In part 1**, you will fine-tune a BERT-style model on the AG News dataset and evaluate its performance. You can find a tutorial for loading BERT and fine-tuning [here](https://huggingface.co/docs/transformers/training). For simplicity, I recommend using the [Hugging Face Transformers library](https://huggingface.co/docs/transformers/index).You're welcome to use a different framework if you prefer.\n","\n","**In part 2**, instead of fine-tuning a BERT-style model directly, you will use the representations from the BERT-style model as input to a linear classifier. Does this approach perform better or worse?\n","\n","For both part 1 and part 2, your goal is to achieve a test accuracy above the specified thresholds. You won’t have access to the test labels—just like in real-world applications!\n","\n","**Tips about fine-tuning**\n","\n","* Data preprocessing: raw text data should be tokenized before being fed to the model as batches during trainig.\n","* Hyperparameter choices: Experiment with settings such as learning rate, warmup ratio, optimizer, number of training steps, and batch size.\n","* Avoid overfitting: remember that your fine-tuned model will be evaluated on the test set!\n","\n","\n","**!! IMPORTANT NOTE !!**\n","\n","You are free to explore and implement the training code however you want to maximize the model performance. However, please put the code you're running under `if __name__ == '__main__':` so that the particular training step is not run when we later evaluate your final script! Otherwise you may fail the Markus tests due to timeout.\n","\n","```\n","if __name__ == '__main__':\n","    # your training code to fine-tune the model\n","    ...\n","```"]},{"cell_type":"markdown","metadata":{"id":"gYmukJqi_qQl"},"source":["# Part 1 (4 points)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edPVKxZtQ0u2","outputId":"0845cf0e-11f7-406c-b942-f3b25423c7d4","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.35.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"]}],"source":["if __name__ == '__main__':\n","    # !pip install datasets\n","    # !pip install evaluate\n","    # !pip install -U sentence-transformers\n","\n","    from datasets import load_dataset, DatasetDict\n","    from transformers import AutoTokenizer\n","    from torch.utils.data import DataLoader\n","    import torch\n","    import evaluate\n","\n","    from sentence_transformers import SentenceTransformer\n","    from sklearn.linear_model import LogisticRegression\n","    import joblib\n","\n","    torch.cuda.empty_cache()\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    ################# Import additional packages you need #################\n","    #####################################################################################\n","    pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwHmIaTUQZYy"},"outputs":[],"source":["################## HELPER CODE FOR SAVING RELEVANT FILES ##################\n","if __name__ == '__main__':\n","    def in_colab():\n","        try:\n","            import google.colab\n","            return True\n","        except ImportError:\n","            return False\n","\n","    if in_colab():\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        SAVE_PATH = '/content/drive/MyDrive'\n","    else:\n","        SAVE_PATH = '.'"]},{"cell_type":"markdown","metadata":{"id":"MenK8lkBOq3g"},"source":["## Part 1.a\n","\n","Fine-tune [TinyBERT](https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D) on AG News and evaluate the results. You can find a tutorial for loading BERT and fine-tuning [here](https://huggingface.co/docs/transformers/training). In that tutorial, you will need to change the dataset from `\"yelp_review_full\"` to the correct dataset path and the model from `\"bert-base-uncased\"` to `\"huawei-noah/TinyBERT_General_4L_312D\"`. You'll also need to modify the code since AG New is a four-class classification dataset (unlike the Yelp Reviews dataset, which is a five-class classification dataset).\n","\n","**TODO**\n","* After fine-tuning the model, save model predictions on the test set to *part1_tiny_bert_model_test_prediction.csv*. The csv file should contain \"index\" columns, corresponding to the unique sample index, and \"pred\" column, the model prediction on that sample. Your model should achieve >= 80% on the test accuracy to receive a full mark.\n","\n","```\n","index, pred\n","0,model_pred_value_0\n","1,model_pred_value_1\n","2,model_pred_value_2\n","...\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3Zr_lHDWVTU"},"outputs":[],"source":["######################## DO NOT MODIFY THE CODE ########################\n","if __name__ == '__main__':\n","    dataset = load_dataset('r-three/ag_news_subset')\n","    model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=4)\n","    tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n","    print(dataset[\"train\"][100])\n","#########################################################################"]},{"cell_type":"markdown","source":["**Your trianing code here...**"],"metadata":{"id":"aWZ8R0IEbz44"}},{"cell_type":"markdown","source":["If your prediction is saved in pandas dataframe, you can do something like:\n","```\n","if __name__ == '__main__':\n","   part1_tiny_bert_pred.to_csv(f\"{SAVE_PATH}/part1_tiny_bert_model_test_prediction.csv\", index=False)\n","```"],"metadata":{"id":"SHieq6Luch21"}},{"cell_type":"markdown","metadata":{"id":"9JTLJgvUIT46"},"source":["## Part 1.b\n","\n","For this section, choose a different pre-trained BERT-style model from the [Hugging Face Model Hub](https://huggingface.co/models) and fine-tune it. There are tons of options - part of the homework is navigating the hub to find different models! I recommend picking a model that is smaller than BERT-Base (as TinyBERT is) just to make things computationally cheaper. Is the final validation accuracy higher or lower with this other model?\n","\n","**TODO**\n","* As in part 1.a, save model predictions on the test set to *part1_hf_bert_model_test_prediction.csv*. The csv file should contain \"index\" columns, corresponding to the unique sample index, and \"pred\" column, the model prediction on that sample. Your model should achieve >=80% on the test accuracy to receive a full mark."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAw02tJzJDme"},"outputs":[],"source":["if __name__ == '__main__':\n","    ############### YOUR CODE ###############\n","    # TODO: find a new HF BERT based model from HuggingFace and load it.\n","    HF_BERT_BASED_MODEL = \"\"\n","    #########################################"]},{"cell_type":"markdown","source":["**Your training code here...**"],"metadata":{"id":"QP_4e2M4cSz3"}},{"cell_type":"markdown","source":["Similarly, you can consider something like:\n","\n","```\n","if __name__ == '__main__':\n","   part1_hf_bert_pred.to_csv(f\"{SAVE_PATH}/part1_hf_bert_model_test_prediction.csv\", index=False)\n","```"],"metadata":{"id":"jOyGA96Dc65h"}},{"cell_type":"markdown","metadata":{"id":"8Yt5QSetH7Vu"},"source":["# Part 2 (2.5 points)\n","\n","Instead of fine-tuning the full model on a target dataset, it's also possible to use the output representations from a BERT-style model as input to a linear classifier and *only* train the classifier (leaving the rest of the pre-trained parameters fixed). You can do this easily using the [`sentence-transformers`](https://www.sbert.net/) library. Using `sentence-tranformers` gives you back a fixed-length representation of a given text sequence. To achieve this, you need to\n","1. Pick a pre-trained sentence Transformer.\n","2. Load the AG News dataset and feed the text from each example into the model.\n","3. Train a linear classifier on the representations.\n","4. Evaluate performance on the validation set.\n","\n","For the second step, you can learn more about how to use Hugging Face datasets [here](https://huggingface.co/docs/datasets/index). For the third and fourth step, it's possible to either do this directly in PyTorch, or collect the learned representations and use them as feature vectors to train a linear classifier in any other library (e.g. [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html)). For this homework, you will implement the second approach.\n","\n","After you complete the above steps, is the accuracy on the validation set higher or lower using a fixed sentence Transformer?\n","\n","**TODO**:\n","* Complete the `encode_data` function: the function embeds each text sample into an output representation using the provided sentence encoder. The function is called to map a text data sample to the model representation, as shown below:\n","```\n","dataset.map(lambda x: encode_data(sen_model, x), batched=True)\n","```\n","* Train a Logistic Regression classifier: use sklearn.linear_model.LogisticRegression to fit the model on the encoded text data.\n","* Save your trained model: After training, saved teh fitted logistic regression model as `sentence_encoder_classification.pkl`. Your model should achieve >=85% on the test accuracy to receive a full mark."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cym2hkG1WbVE"},"outputs":[],"source":["def encode_data(model, x):\n","    \"\"\"Takes the model and the dataset object\n","        Returns a dictionary consisting of \"encoded_input\" and \"label\" as keys.\n","        - \"encoded_input\" contains the tokenized text features produced by the sentence transformer.\n","        - \"label\" is the target class label for each example.\n","        encoded_input is the encoded text input, and label is the target label.\n","        NOTE: Please assume the dataset object is the original one loaded via\n","              load_dataset('r-three/') for reproducibility.\n","              Which means if you want to create additional features to create the encoded_input,\n","              do so within this function.\n","    \"\"\"\n","    ####################### YOUR CODE ##########################\n","    # TODO: encoded_input\n","    d = {'encoded_input': None, 'label': None}\n","    return d\n","    ############################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kB836CIZqulu"},"outputs":[],"source":["########### PUT YOUR MODEL HERE ###########\n","SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'\n","###########################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQvqBakJjJ7b"},"outputs":[],"source":["########### DO NOT CHANGE THIS CODE ###########\n","if __name__ == \"__main__\":\n","\n","    sen_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n","    # Prepare the dataset\n","    tokenized_dataset = dataset.map(lambda x: encode_data(sen_model, x), batched=True)\n","    print(tokenized_dataset['train'][100])\n","    X_train = np.stack([np.array(x['encoded_input']) for x in tokenized_dataset['train']])\n","    X_val = np.stack([np.array(x['encoded_input']) for x in tokenized_dataset['validation']])\n","    y_train = np.stack([np.array(x['label']) for x in tokenized_dataset['train']])\n","    y_val = np.stack([np.array(x['label']) for x in tokenized_dataset['validation']])\n","\n","    print(X_train.shape)\n","    print(X_val.shape)\n","    print(y_train.shape)\n","    print(y_val.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fk2cKDK_bk_y"},"outputs":[],"source":["########### COMPLETE THE FOLLOWING LOGISTIC REGRESSION CODE ###########\n","if __name__ == \"__main__\":\n","    classifier = LogisticRegression(max_iter=1000)\n","    # Your logistic regression training code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_j3mx8IMvNG"},"outputs":[],"source":["######################## TO SUBMIT ########################\n","if __name__ == \"__main__\":\n","    joblib.dump(classifier, f\"{SAVE_PATH}/sentence_encoder_classification.pkl\")\n","    # test if it loads as expected\n","    # loaded_model = joblib.load(f\"{SAVE_PATH}/sentence_encoder_classification.pkl\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"10ikUtpX5KEpjpMzTlEu32Thy35_RxrXE","timestamp":1761950913769}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}