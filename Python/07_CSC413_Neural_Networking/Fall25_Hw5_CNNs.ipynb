{"cells":[{"cell_type":"markdown","id":"c552ddd6","metadata":{"id":"c552ddd6"},"source":["# Homework 5 - Convolutional Neural Networks\n","CSC413/2516: Neural Networks and Deep Learning\n","\n","As with previous homeworks, replace \"## Your Code\" lines with your implementation.\n","\n","\n","In this homework you will implement the convolution operation from scratch, do convolution arithmetic (receptive field, number of parameters), and implement a convolutional neural network in PyTorch. While you are not allowed to use PyTorch in the first section, you are allowed to use it in the third part of the homework.\n","- Convolution from scratch (1 point)\n","- Convolution arithmetic (2 points)\n","- Convolutional neural network in PyTorch (3.5 points)\n","\n","Make sure to connect to GPU for the training part (Runtime -> Change runtime -> Select T4 GPU)\n","\n","Current version: 1.0.2\n","\n","Changelog\n","- 1.0.1: Updated grading scheme `convolution_2d_as_matrix_vector` is now 0.25 points instead of 0.3, CNN __init__ is now 0.45 points instead of 0.4\n","- 1.0.2 (Oct 8th): Added seed fixing\n"]},{"cell_type":"code","execution_count":null,"id":"baad1a76","metadata":{"id":"baad1a76"},"outputs":[],"source":["import importlib.util\n","\n","packages = ['torch', 'torchvision', 'datasets']\n","missing = [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n","\n","if missing:\n","    import subprocess\n","    import sys\n","    print(f\"Installing missing packages: {', '.join(missing)}\")\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing)"]},{"cell_type":"code","execution_count":null,"id":"b292968b","metadata":{"id":"b292968b"},"outputs":[],"source":["import numpy as np\n","from typing import Dict, Tuple, List\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","\n","from datasets import load_dataset\n","\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"markdown","id":"47e7339f","metadata":{"id":"47e7339f"},"source":["## Implement the convolution operation from scratch"]},{"cell_type":"code","execution_count":null,"id":"f41ceec2","metadata":{"id":"f41ceec2"},"outputs":[],"source":["def output_shape_of_conv(\n","    image_height: int,\n","    image_width: int,\n","    kernel_height: int,\n","    kernel_width: int,\n","    padding: int,\n","    stride: int,\n",") -> tuple[int, int]:\n","    \"\"\"Compute the output shape of a convolutional layer given the input shape and the kernel shape.\"\"\"\n","    out_height, out_width = 0, 0\n","    ##########################################\n","    ## TODO: Implement the output shape computation, 0.2 points\n","\n","    ##########################################\n","    return out_height, out_width\n","\n","\n","def convolution(\n","    image: np.typing.ArrayLike,\n","    kernel: np.typing.ArrayLike,\n","    padding: int = 0,\n","    stride: int = 1,\n",") -> np.typing.ArrayLike:\n","    \"\"\"Perform a convolution operation on the image with the kernel using padding and stride. Use the traditional definition of convolution (not cross-correlation).\n","    Images are grayscale and of shape  (H, W), H=height, W=width.\"\"\"\n","    res = np.zeros(\n","        output_shape_of_conv(\n","            image.shape[0],\n","            image.shape[1],\n","            kernel.shape[0],\n","            kernel.shape[1],\n","            padding,\n","            stride,\n","        )\n","    )\n","    ##########################################\n","    ## TODO: Implement the convolution operation, 0.6 points\n","    ## Your code will be tested with different padding and stride values\n","    ## 1. Apply padding to the image\n","    ## Hint: You can use np.pad\n","\n","    ## 2. Implement the convolution operation on the padded image\n","    ## Hint: Either use np.flip or carefully select the indices\n","\n","    ##########################################\n","    return res\n"]},{"cell_type":"markdown","id":"6adeb926","metadata":{"id":"6adeb926"},"source":["### Plus Detector\n","Implement a 3x3 kernel that detects a \"+\" shape (bright cross on dark background). You will receive full points if your kernel outputs the maximum response at the center of the plus(es)."]},{"cell_type":"code","execution_count":null,"id":"be008654","metadata":{"id":"be008654","colab":{"base_uri":"https://localhost:8080/","height":504},"executionInfo":{"status":"ok","timestamp":1759334207011,"user_tz":240,"elapsed":790,"user":{"displayName":"Gül Sena Altıntaş","userId":"03454584352454095776"}},"outputId":"398927e7-7f67-41a3-b5d8-a03573f1e9a0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4173827967.py:21: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n","  axs[1].imshow(result, cmap=\"gray\")\n","/tmp/ipython-input-4173827967.py:21: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n","  axs[1].imshow(result, cmap=\"gray\")\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Create a test image with a plus sign\n","test_image = np.array(\n","    [\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n","        [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1],\n","        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","    ],\n","    dtype=np.float32,\n",")\n","## TODO: Implement the plus detector, 0.2 points\n","plus_detector = np.zeros((3, 3), dtype=np.float32)\n","\n","if __name__ == \"__main__\":\n","    result = convolution(test_image, plus_detector, padding=1, stride=1)\n","    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n","    axs[0].imshow(test_image, cmap=\"gray\")\n","    axs[0].set_title(\"Test Image\")\n","    axs[1].imshow(result, cmap=\"gray\")\n","    axs[1].set_title(\"Result\")\n","    plt.show()\n"]},{"cell_type":"markdown","id":"49d6fef1","metadata":{"id":"49d6fef1"},"source":["## Convolutional Neural Network\n"]},{"cell_type":"markdown","id":"01cabb79","metadata":{"id":"01cabb79"},"source":["## Warm-up\n","In this section, you will be asked to compute the parameter count and the receptive field of a given convolutional neural network.\n"]},{"cell_type":"code","execution_count":null,"id":"514acbac","metadata":{"id":"514acbac"},"outputs":[],"source":["def conv_layer_num_weights(\n","    k1: int, k2: int, in_channels: int, out_channels: int\n",") -> int:\n","    params = 0\n","    ##########################################\n","    ## TODO: Implement the parameter count (for weights) computation for a given convolutional layer, 0.05 points\n","\n","    ##########################################\n","    return params\n","\n","def conv_layer_num_biases(\n","    k1: int, k2: int, in_channels: int, out_channels: int\n",") -> int:\n","    params = 0\n","    ##########################################\n","    ## TODO: Implement the parameter count (for biases) computation for a given convolutional layer, 0.05 points\n","\n","    ##########################################\n","    return params\n"]},{"cell_type":"markdown","id":"31300ede","metadata":{"id":"31300ede"},"source":["In the following cell, we will ask you to compute the receptive field and parameter count for three networks. For the parameter count, you are free to use the `conv_layer_num_weights` and `conv_layer_num_biases` functions. When computing the receptive field, you are free to come up with a general formula or compute it by hand, we will only test the final answer.\n"]},{"cell_type":"code","execution_count":null,"id":"6e1d304b","metadata":{"id":"6e1d304b"},"outputs":[],"source":["########################################################\n","#### Network 1\n","########################################################\n","# Imagine you have a 3-layer CNN, with the following architecture:\n","## Layer 1: Conv2d(3, 16, 3, 1), i.e.  out_channels=16, in_channels=3, kernel_size=3, stride=1, padding=0\n","## Layer 2: Conv2d(16, 32, 3, 1), i.e. out_channels=32, in_channels=16, kernel_size=3, stride=1, padding=0\n","## Layer 3: Conv2d(32, 64, 3, 1), i.e. out_channels=64, in_channels=32, kernel_size=3, stride=1, padding=0\n","def get_parameters_of_network1() -> Dict[str, int]:\n","    ## TODO: Compute the number of parameters in the network above, assume we don't use batch normalization or dropout.\n","    ## 0.25 points\n","    parameters = {\n","        \"layer_1_weights\": 0,\n","        \"layer_1_biases\": 0,\n","        \"layer_2_weights\": 0,\n","        \"layer_2_biases\": 0,\n","        \"layer_3_weights\": 0,\n","        \"layer_3_biases\": 0,\n","    }\n","    return parameters\n","\n","\n","def get_receptive_field_for_network1() -> Dict[str, Tuple[int, int]]:\n","    ## TODO: Change the following values to reflect the receptive field of the network above, provide it in the form of (height, width) of the receptive field\n","    receptive_fields = {\n","        \"after_layer_1\": (0, 0),\n","        \"after_layer_2\": (0, 0),\n","        \"after_layer_3\": (0, 0),\n","    }\n","    return receptive_fields\n","\n","\n","########################################################\n","#### Network 2\n","########################################################\n","## TODO: What if the stride is 2x3 at each layer in the above network?\n","## Layer 1: Conv2d(3, 16, 3, (2, 3)), i.e. kernel_size=3, stride=(2, 3), padding=0\n","## Layer 2: Conv2d(16, 32, 3, (2, 3)), i.e. kernel_size=3, stride=(2, 3), padding=0\n","## Layer 3: Conv2d(32, 64, 3, (2, 3)), i.e. kernel_size=3, stride=(2, 3), padding=0\n","def get_receptive_field_for_network1_with_stride_2x3() -> Dict[str, Tuple[int, int]]:\n","    ## TODO: Change the following values\n","    receptive_fields = {\n","        \"after_layer_1\": (0, 0),\n","        \"after_layer_2\": (0, 0),\n","        \"after_layer_3\": (0, 0),\n","    }\n","    return receptive_fields\n","\n","\n","def get_parameters_of_network1_with_stride_2x3() -> Dict[str, int]:\n","    ## TODO: Compute the number of parameters in the network above, assume we don't use batch normalization or dropout.\n","    parameters = {\n","        \"layer_1_weights\": 0,\n","        \"layer_1_biases\": 0,\n","        \"layer_2_weights\": 0,\n","        \"layer_2_biases\": 0,\n","        \"layer_3_weights\": 0,\n","        \"layer_3_biases\": 0,\n","    }\n","    return parameters\n","\n","\n","########################################################\n","#### Network 3\n","########################################################\n","# Imagine you have a 3-layer CNN, with the following architecture:\n","## Layer 1: Conv2d(3, 16, 5, 1) -> ReLU -> BN, i.e. kernel_size=5, stride=1, padding=1\n","## Layer 2: Conv2d(16, 32, 3, 2) -> ReLU -> BN, i.e. kernel_size=3, stride=2, padding=1\n","## Layer 3: Conv2d(32, 64, 3, 1) , i.e. kernel_size=3, stride=1, padding=1\n","def get_parameters_of_network3() -> Dict[str, int]:\n","    ## TODO: Compute the number of parameters in the network above\n","    parameters = {\n","        \"layer_1_conv_weights\": 0,\n","        \"layer_1_conv_biases\": 0,\n","        \"layer_1_bn_weights\": 0,\n","        \"layer_1_bn_biases\": 0,\n","        \"layer_1_relu\": 0,\n","        \"layer_2_conv_weights\": 0,\n","        \"layer_2_conv_biases\": 0,\n","        \"layer_2_bn_weights\": 0,\n","        \"layer_2_bn_biases\": 0,\n","        \"layer_2_relu\": 0,\n","        \"layer_3_conv_weights\": 0,\n","        \"layer_3_conv_biases\": 0,\n","    }\n","    return parameters\n","\n","\n","def get_receptive_field_for_network3() -> Dict[str, int]:\n","    ## TODO: Change the following values\n","    receptive_fields = {\n","        \"after_layer_1\": (0, 0),\n","        \"after_layer_2\": (0, 0),\n","        \"after_layer_3\": (0, 0),\n","    }\n","    return receptive_fields\n"]},{"cell_type":"markdown","source":["#### Convolution as a Matrix-Vector Product\n","\n","We can also implement the convolution operation as a matrix product. This alternative view is useful for understanding how convolutions work mathematically and connects to how they're implemented efficiently in deep learning frameworks.\n","\n","In this formulation, we reshape the input image into a column vector and construct a special matrix (called a Toeplitz matrix) where each row extracts the pixels that the kernel would multiply at one output position. The matrix-vector product then computes all output values simultaneously.\n","\n"],"metadata":{"id":"m4L_KlEE_EqD"},"id":"m4L_KlEE_EqD"},{"cell_type":"code","execution_count":null,"id":"b2b748f0","metadata":{"id":"b2b748f0"},"outputs":[],"source":["########################################################\n","#### 1D Convolution as Matrix-Vector Product\n","########################################################\n","def convolution_1d_as_matrix_vector() -> np.ndarray:\n","    \"\"\"\n","    Input: x = [x1, x2, x3, x4]\n","    Kernel: k = [1, 0.5]\n","    No padding, stride=1\n","    \"\"\"\n","    ## TODO: Write the convolution kernel as a matrix, 0.15 points\n","    matrix = np.array([])\n","    return matrix\n","\n","\n","########################################################\n","#### 2D Convolution as Matrix-Vector Product\n","########################################################\n","def convolution_2d_as_matrix_vector() -> np.ndarray:\n","    \"\"\"\n","    Input: 4x4 image (Hint: flattened to 16x1 vector)\n","    Kernel: [[1, 1], [1, 1]]\n","    No padding, stride=2\n","    \"\"\"\n","    ## TODO: Write the convolution kernel as a matrix, 0.25 points\n","    matrix = np.array([])\n","\n","    return matrix"]},{"cell_type":"markdown","id":"b539aa9e","metadata":{"id":"b539aa9e"},"source":["## Implement a CNN for the subset of CIFAR-10 dataset\n","In this section, you will implement a convolutional neural network using PyTorch. One of the famous datasets in the computer vision community is the CIFAR-10 dataset, and it was developed here at UofT! It consists of 60000 32x32 color images in 10 classes, with 6000 images per class. For this assignment, we will use a small subset of the CIFAR-10 Dataset with only 5 classes, available [here](https://huggingface.co/datasets/r-three/cifar10-5class-5k).\n","\n","### Your Task\n","Build a CNN that achieves **>60% validation accuracy**. You'll implement:\n","1. CNN architecture (convolutional layers, batch norm, pooling, FC layer)\n","2. Forward pass\n","3. Optimizer setup\n","4. Training and evaluation functions"]},{"cell_type":"markdown","id":"3f8ad9ad","metadata":{"id":"3f8ad9ad"},"source":["### Set-up\n"]},{"cell_type":"code","execution_count":null,"id":"56d49526","metadata":{"id":"56d49526"},"outputs":[],"source":["## don't change this cell\n","class CIFAR5Dataset(Dataset):\n","    \"\"\"Custom Dataset wrapper for CIFAR-5 (downsampled from CIFAR-10)\"\"\"\n","\n","    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\"]\n","\n","    def __init__(self, hf_dataset, transform=None):\n","        self.dataset = hf_dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        image = item[\"img\"]\n","        label = item[\"label\"]\n","\n","        # Convert to RGB\n","        if image.mode != \"RGB\":\n","            image = image.convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","def visualize_cifar5(loader, num_samples=16):\n","    \"\"\"\n","    Visualize samples from CIFAR5 dataset\n","    \"\"\"\n","    dataiter = iter(loader)\n","    images, labels = next(dataiter)\n","\n","    # make_grid expects CHW, returns CHW, so permute after\n","    grid = torchvision.utils.make_grid(images[:num_samples])\n","    plt.imshow(grid.permute(1, 2, 0))\n","    plt.axis(\"off\")\n","    plt.show()\n"]},{"cell_type":"markdown","id":"71b8967e","metadata":{"id":"71b8967e"},"source":["### Implement CNN\n","When implementing your CNN, you'll define layers in `__init__()` and use them in `forward()`. PyTorch handles the backward pass automatically\n"]},{"cell_type":"code","execution_count":null,"id":"f12a3dc9","metadata":{"id":"f12a3dc9"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_classes=5):\n","        super(CNN, self).__init__()\n","        ##########################################\n","        ## TODO: Define your CNN architecture, 0.45 points\n","        ## The input tensor is of shape (batch_size, 3, 32, 32)\n","        ## For all conv layers assume k=3, s=1, p=1\n","        ## Use k=2 and s=2 for pooling layers\n","        ##\n","        ## Suggested structure:\n","        ## - Conv layer 1: 3 -> 16 channels\n","        ## - BN + ReLU + MaxPool\n","        ## - Conv layer 2: 16 -> 32 channels\n","        ## - BN + ReLU + MaxPool\n","        ## - Flatten\n","        ## - FC layer: ? -> num_classes\n","        ##########################################\n","        self.conv1 = None\n","        self.bn1 = None\n","        self.pool1 = None\n","\n","        self.conv2 = None\n","        self.bn2 = None\n","        self.pool2 = None\n","\n","        self.fc = None\n","        ## Optional: either define them here or use alternative functions in the forward pass\n","        self.flatten = None\n","        self.relu = None\n","        ##########################################\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        res = x\n","        ##########################################\n","        ## TODO: Implement the forward pass, 0.45 points\n","        ## Input tensor of shape (batch_size, 3, 32, 32)\n","        ## PyTorch will build the computational graph and keep track of the operations\n","\n","        ##########################################\n","        return res\n"]},{"cell_type":"markdown","id":"a6917517","metadata":{"id":"a6917517"},"source":["## Training and Optimization\n","Make sure to optimize your args and \"num_epochs\" is less than or equal to 10. Training test case will timeout after 5 minutes, which is more than enough to achieve 65+% accuracy on this dataset."]},{"cell_type":"code","execution_count":null,"id":"5b4627f5","metadata":{"id":"5b4627f5"},"outputs":[],"source":["args = {\n","    \"batch_size\": 1,\n","    \"learning_rate\": 1,\n","    \"num_epochs\": 1,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    \"optimizer\": \"SGD\",\n","    \"momentum\": 0.9,\n","    \"weight_decay\": 0,\n","    \"eval_interval\": 100,\n","    \"num_classes\": 5,\n","}"]},{"cell_type":"code","execution_count":null,"id":"305b4427","metadata":{"id":"305b4427"},"outputs":[],"source":["def setup_optimizer(model, args):\n","    optimizer = None\n","    ##########################################\n","    ## TODO: Create the optimizer setup based on your args, 0.15 points\n","    ## Hint: You can use torch.optim.SGD, torch.optim.Adam, torch.optim.AdamW,\n","    ## torch.optim.RMSprop, etc.\n","    ## You are free to use any optimizer you want, with any hyperparameters you want,\n","    ## make sure to update `args` accordingly.\n","\n","    ##########################################\n","    return optimizer\n","\n","\n","def accuracy(output, target):\n","    acc = 0\n","    ##########################################\n","    ## TODO: Compute the accuracy of the model, should be between 0 and 1, 0.25 points\n","    ## Hint: output shape is (bs, n_classes)\n","    ## target shape: (bs, 1) or (bs, )\n","    ## Note: This funcion is just for you to practice, it will not be used again\n","\n","    # ##########################################\n","    return acc\n"]},{"cell_type":"code","execution_count":null,"id":"bf4879be","metadata":{"id":"bf4879be"},"outputs":[],"source":["@torch.no_grad()\n","def eval(model, test_loader, args):\n","    \"\"\"\n","    Evaluate the model on the test set.\n","\n","    Key PyTorch concepts used here:\n","    - @torch.no_grad(): Disables gradient computation to save memory and speed up evaluation.\n","      Since we're not training, we don't need gradients. This is more efficient than computing\n","      gradients and just not using them. And we should never train on the test/validation set!\n","    - model.eval(): Sets the model to evaluation mode. This affects layers like BatchNorm and\n","      Dropout (though we don't use Dropout here). In eval mode, BatchNorm uses running statistics\n","      instead of batch statistics.\n","    \"\"\"\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        ## When using a gpu, we need to make sure inputs are on the same device as the model\n","        data, target = data.to(args[\"device\"]), target.to(args[\"device\"])\n","        loss = torch.tensor([0])\n","        ##########################################\n","        ## 0.8 points\n","        ## TODO: Compute the loss of the model\n","        ## (Hint: you first need to compute the output of the model)\n","\n","        ## TODO: Keep track of the accuracy of the model\n","        ## (Hint: use `correct` variable to keep track of the total number of\n","        ## correctly predicted data points), should be similar to your `accuracy` implementation\n","\n","        ##########################################\n","        test_loss += loss.item()\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy\n","\n","\n","\n","def torch_step(model, batch_data, batch_target, optimizer):\n","    \"\"\"\n","    Perform one training step (forward pass + backward pass + parameter update).\n","\n","    Key PyTorch concepts:\n","    - model.train(): Sets model to training mode (opposite of model.eval()). BatchNorm\n","      will compute statistics from the current batch.\n","    - optimizer.zero_grad(): Clears gradients from the previous step. PyTorch accumulates\n","      gradients by default, so we must clear them before each backward pass.\n","    - loss.item(): Extracts the scalar value from the loss tensor (converts from tensor to Python float).\n","      Without .item(), you'd return a tensor which keeps the entire computation graph in memory.\n","    \"\"\"\n","    model.train()\n","    optimizer.zero_grad()\n","    loss = torch.tensor([0])\n","    ##########################################\n","    ## TODO: Perform a single step of gradient descent, 0.4 points\n","    ## 1. Compute model output\n","    ## 2. Compute loss: (Hint: https://docs.pytorch.org/docs/stable/nn.functional.html#loss-functions)\n","    ## 3. Compute gradients\n","    ## 4. Update parameters. The optimizer knows about all model parameters from setup_optimizer().\n","\n","    ##########################################\n","    return loss.item()\n"]},{"cell_type":"code","execution_count":null,"id":"dd649468","metadata":{"id":"dd649468"},"outputs":[],"source":["def train(model, train_loader, validation_loader, args) -> Tuple[List[float], float, float]:\n","    model.train()\n","    optimizer = setup_optimizer(model, args)\n","    train_losses = []\n","\n","    current_step = 0\n","    for epoch in range(args[\"num_epochs\"]):\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            data, target = data.to(args[\"device\"]), target.to(args[\"device\"])\n","            loss = torch_step(model, data, target, optimizer)\n","            train_losses.append(loss)\n","\n","            if batch_idx % 10 == 0:\n","                print(\n","                    f\"Epoch {epoch} [{batch_idx}/{len(train_loader)}] Loss: {loss:.4f}\"\n","                )\n","\n","            current_step += 1\n","            if current_step % args[\"eval_interval\"] == 0:\n","                val_loss, val_accuracy = eval(model, validation_loader, args)\n","                print(\n","                    f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}\"\n","                )\n","\n","    return train_losses, val_loss, val_accuracy\n"]},{"cell_type":"markdown","id":"b814dc1d","metadata":{"id":"b814dc1d"},"source":["## Train and achieve >60% accuracy on the validation set (1 point)\n","Make sure to optimize your `args` above.\n","\n","### Data Augmentation and Preprocessing\n","\n","Before training, we need to preprocess our images. **Data augmentations** create variations of training images (flips, crops) to help the model generalize better and prevent overfitting. We only augment training data - test data remains unchanged for consistent evaluation.\n","\n","**Why normalize?** Neural networks train better when inputs are centered around 0.\n"]},{"cell_type":"code","execution_count":null,"id":"f619e4a3","metadata":{"id":"f619e4a3"},"outputs":[],"source":["# Training transforms: augmentation + normalization\n","transform_train = transforms.Compose(\n","    [\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ]\n",")\n","\n","# Test transforms: only normalization (no augmentation)\n","transform_test = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ]\n",")\n","## Don't change the remaining code\n","if __name__ == \"__main__\":\n","    # Load the downsampled CIFAR-10 dataset\n","    print(\"Loading dataset...\")\n","    dataset = load_dataset(\"r-three/cifar10-5class-5k\")\n","\n","    # Class names\n","    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\"]\n","    num_classes = len(class_names)\n","\n","    print(f\"\\nDataset Info:\")\n","    print(f\"Train samples: {len(dataset['train'])}\")\n","    print(f\"Test samples: {len(dataset['test'])}\")\n","    print(f\"Classes: {class_names}\")\n","\n","    # Create datasets\n","    train_dataset = CIFAR5Dataset(dataset[\"train\"], transform=transform_train)\n","    test_dataset = CIFAR5Dataset(dataset[\"test\"], transform=transform_test)\n","\n","    # Create dataloaders\n","    train_loader = DataLoader(\n","        train_dataset, batch_size=args[\"batch_size\"], shuffle=True\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=args[\"batch_size\"], shuffle=False)\n","    visualize_cifar5(test_loader)\n","\n","    print(\"Creating model...\")\n","    model = CNN(num_classes=args[\"num_classes\"])\n","    # Move the model to the device\n","    model = model.to(args[\"device\"])\n","\n","    train_losses, val_loss, val_acc = train(model, train_loader, test_loader, args)\n","    plt.plot(train_losses)"]},{"cell_type":"markdown","source":["### Reflection Questions\n"," (Not graded)\n","\n","After implementing convolutions from scratch and as matrix operations, consider these questions:\n","\n","**Efficiency and Parameters:**\n","1. A fully connected layer connecting a 32 $\\times$ 32 RGB image to 64 output features has how many parameters? Compare this to a Conv2d(3, 64, 3) layer. Why is the convolutional layer so much more parameter-efficient?\n","\n","2. What makes convolutional layers \"translation equivariant\"? Why is this property useful for image tasks but might be less relevant for other types of data?\n","\n","**Storage and Memory:**\n","\n","3. Consider storing intermediate activations during backpropagation. For a 224 $\\times$ 224 input image passing through a Conv2d(3, 64, 3) versus a fully connected layer with 64 outputs - which requires more memory to store activations?\n","\n","5. When implementing convolution as a matrix-vector product, the matrix is very sparse (mostly zeros). Modern implementations don't actually create this full matrix. Why not? What memory savings does this provide for a 1000 $\\times$ 1000 image with a 3 $\\times$ 3 kernel?\n","\n","**Architecture Design:**\n","\n","6. Why do CNNs typically use small kernels (3 $\\times$ 3, 5 $\\times$ 5) stacked in multiple layers rather than one large kernel (e.g., 11 $\\times$ 11) in a single layer? Consider both receptive field and parameter count.\n"],"metadata":{"id":"pQYerYs3_uD8"},"id":"pQYerYs3_uD8"},{"cell_type":"markdown","source":["# Collaboration / External Help\n","Disclose any help you used (LLM usage, blogs, search, Github links, etc) and collaborations with your classmates. If you  completed the homework on your own, you can leave this part empty.\n","\n","> TODO"],"metadata":{"id":"P2XHgIgcAXae"},"id":"P2XHgIgcAXae"},{"cell_type":"code","execution_count":null,"id":"300d5941","metadata":{"id":"300d5941"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[{"file_id":"1nYShZKgMM_s_q-s1h2Hpk9Guu4QVAXSm","timestamp":1761950846179}]}},"nbformat":4,"nbformat_minor":5}