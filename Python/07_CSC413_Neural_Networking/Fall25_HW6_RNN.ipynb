{"cells":[{"cell_type":"markdown","metadata":{"id":"t2hPDx3Tvh0v"},"source":["# Homework 6\n","\n","In this homework, you will train and experiment with a \"char-RNN\" -— a language model that predicts the next character in a sequence (see [this famous blog post by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)). Before you start on the rest of the homework, please give the blog post a read! Unlike Karpathy’s original implementation, you will be building your own version using modern PyTorch modules. Rather than relying on nn.RNN or nn.GRU directly, you will implement the recurrent computations manually to see exactly how hidden states are updated over time.\n","\n","For this homework, we are going to implement the following:\n","\n","**Data class**\n","1. Vocab: builds a character-level vocabulary for encoding and decoding the given text.\n","1. CharDataset: cleans the raw text and converts it into sequences of fixed length, using the Vocab class for tokenization.\n","\n","**Model class**\n","1. RNNScratch: a “vanilla” RNN implementation that performs hidden state updates.\n","1. GRUScratch: A gated recurrent unit (GRU) version that adds gating mechanisms.\n","1. RNNLMScratch: a wrapper that combines the RNN cell (either vanilla or GRU) with an output layer for predicting the next character.\n","1. Trainer: a simple class for handling training loop, optimization, loss recording, and evaluation.\n","\n","For RNNScratch and GRUScratch, compare their final training/validation loss and the quality of text samples generated. Which one has the higher or lower final training loss? Are the samples from one model more or less realistic?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0HzeKd9y3cv"},"outputs":[],"source":["import re\n","import torch\n","import random\n","import collections\n","import numpy as np\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","from typing import Dict, Tuple, List\n","\n","import os\n","os.environ[\"HF_TOKEN\"] = \"\"\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["######################## DATA LOADING (DO NOT DELETE THE CELL) ##########################\n","if __name__ == '__main__':\n","\n","    ds = load_dataset(\"r-three/shakespeare-sonnet-dialogue-blob\")\n","    shakes_ds = ' '.join(ds['train']['text'])\n","\n","    print(shakes_ds[:499])\n","    print(\"\\n\")\n","\n","    print('shakes distinct characters: {}'.format(set(shakes_ds)))\n","    print('shakes total # characters: {}'.format(len(set(shakes_ds))))"],"metadata":{"id":"PTmJLzNl4f8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LW-_7IpjDb4W"},"source":["## Part 1: Text Preprocessing (1.5 points)\n","\n","To train a language model, we first need to convert raw text into a numerical form that the RNN can process. In this homework for building a char-RNN, each individual character is treated as a token. For example, the sentence \"good morning\" consists of a 12-token sequence (including the gap), ['g','o','o','d',' ','m','o','r','n','i','n','g']. Each character is mapped to a unique integer.\n","\n","\n","**TODO**\n","1. Complete the Vocab class: builds a mapping between each character (token) and its integer index.\n","2. Complete the CharDataset class: processes the raw text using the Vocab class; converts the text into sequences of fixed length (num_steps) to form training examples. This can be wrapped by a PyTorch DataLoader to generate mini-batches during training.\n","\n","Usage:\n","\n","```\n","vocab = Vocab(tokens=['a','b','c'])\n","a_index = vocab['a']\n","a_index_tok = vocab.to_tokens(a_index)\n","'a' == a_index_tok # this should return True\n","\n","mydataset = CharDataset(num_steps, raw_text)\n","loader = DataLoader(mydataset, batch_size=4)\n","for b in loader:\n","    # b is a batch of size 4\n","    ...\n","```"]},{"cell_type":"markdown","source":["### Part 1.a: Vocab class\n","\n","Important consideration when designing Vocab\n","* unk() function returns the index for `<unk>` token, which denotes an unknown token.\n","* In your Vocab class, treat any token with frequency count of less than min_freq as `<unk>` token. Hint: you can use collections.Counter\n","* This allows the model using the Vocab class to handle unseen token during inference time."],"metadata":{"id":"QX7p4nyv4nBX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do6coK2Ky3cx"},"outputs":[],"source":["class Vocab:\n","    \"\"\"Vocabulary for raw text data\"\"\"\n","    def __init__(self, tokens=[], min_freq=1):\n","        if tokens and isinstance(tokens[0], list):\n","            tokens = [token for line in tokens for token in line]\n","\n","        ###################### YOUR CODE ####################\n","        # TODO: Map a token to a specific index.\n","        # If the token occurrence is less than min_freq, exclude it from the list of tokens\n","        self.idx_to_token = None\n","        self.token_to_idx = {}\n","        #####################################################\n","\n","    def __len__(self):\n","        ###################### YOUR CODE ####################\n","        # TODO: Return the total token count\n","        return 0\n","        #####################################################\n","\n","    def __getitem__(self, tokens: List[str] | str) -> List[int] | int:\n","        ###################### YOUR CODE ####################\n","        # TODO: Return the corresponding index or list of indices for the given tokens\n","        # If input is List[str], return List[int]. If input is str, return int.\n","        return []\n","        #####################################################\n","\n","    def to_tokens(self, indices: List[int] | int) -> List[str] | str:\n","        ###################### YOUR CODE ####################\n","        # TODO: Given the index or list of indices, map the index back to token(s)\n","        # If input is List[int], return List[str]. If input is int, return str.\n","        return ''\n","        #####################################################\n","\n","    @property\n","    def unk(self):  # Index for the unknown token\n","        return self.token_to_idx['<unk>']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no19K3h7y3cy"},"outputs":[],"source":["######## SIMPLE CHECK ########\n","voc = Vocab(tokens=['a','b','c','d','d'], min_freq=2)\n","print(voc.token_to_idx)\n","# should be True\n","print('d' == voc.to_tokens(voc['d']))"]},{"cell_type":"markdown","source":["### Part 1.b: CharDataset class\n","\n","Now that you have implemented the Vocab class to map characters to indices, we will create a custom Dataset class extends torch.utils.data.Dataset. The core functionality of the PyTorch Dataset is to provide a way to access individual data samples and their corresponding labels. Dataset works in conjunction with a DataLoader, which handles batching, shuffling, and data loading to feed samples to the model during training.\n","\n","In our case, CharDataset will construct the input and target sequences, X and Y, where Y is simply X shifted one position to the right.\n","\n","For instance, given a raw text \"hi, how are you?\", `_preprocess` and `_tokenize` will clean this text and tokenize it (['h' 'i' ',' 'h' 'o' 'w' ' ' ...]) to the corresponding indices ([1 2 3 4 5 6 7...]). If we set num_steps = 3, the resulting input (X) and target sequences (Y) for the character-level RNN would look like this:\n","```\n","X:\n","1 2 3\n","2 3 4\n","3 4 5\n","4 5 6\n","...\n","\n","Y:\n","2 3 4\n","3 4 5\n","4 5 6\n","5 6 7\n","...\n","```\n","\n","**NOTE**: In language modeling, a common practice is to mark the beginning or end of sentences with special tokens such as `<bos>` (beginning of sentence) and `<eos>` (end of sentence). You may also choose to clean the text in other ways depending on your needs. In our case, you can implement your own _preprocess logic to handle any text cleaning or preprocessing logic before buidling the input and target sequences."],"metadata":{"id":"jqKKClxY47tb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5TnlMXCy3cy"},"outputs":[],"source":["class CharDataset(Dataset):\n","    \"\"\"Builds the dataset class handling raw text processing\"\"\"\n","    def __init__(self, num_steps, raw_text):\n","        self.num_steps = num_steps\n","\n","        self.tokens = self._tokenize(self._preprocess(raw_text))\n","        self.vocab = Vocab(self.tokens)\n","        self.corpus = [self.vocab[token] for token in self.tokens]  # convert tokens to corresponding indices\n","\n","        ########################## YOUR CODE ##########################\n","        # TODO: define X and Y input data using corpus\n","        # X is the input sequence and Y is the corresponding labels.\n","        # X, Y should be torch.tensor()\n","        self.X, self.Y = None, None\n","        ###############################################################\n","\n","    def _preprocess(self, text: str):\n","        ########################## YOUR CODE ##########################\n","        # TODO: lowercase the alphabets, and do additional cleaning of the raw text as you want.\n","        # For instance, you can consider replacing the non-alphabets with white space (or not).\n","        return text\n","        ###############################################################\n","\n","    def _tokenize(self, text: str) -> List[str]:\n","        ########################## YOUR CODE ##########################\n","        # TODO: Given an input text, return a list of tokens\n","        # e.g. \"hello world\" -> ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n","        return ['']\n","        ###############################################################\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.Y[idx]\n","\n","    def __len__(self):\n","        return len(self.X)\n"]},{"cell_type":"markdown","metadata":{"id":"uoevqeOrD0P-"},"source":["## Problem 2: Build RNN/GRU model from scratch (3.5 points)\n","\n","Now that you have built the data processing classes to clean the raw text corpus and prepare batches, you will build your own RNN modules from scratch. In this section, you will manually implement both a vanilla RNN and a GRU, along with a simple language modeling wrapper for training the actual RNN-based language model.\n","\n","TODO:\n","1. Complete the **RNNScratch**: A single vanilla RNN layer.\n","2. Complete the **GRUScratch**: A single GRU layer.\n","3. Complete the **RNNLMScratch**: language model wrapper that connects RNN/GRU cell to a embedding lyaer and output layer.\n","\n","NOTE:\n","* Throughout Part 2, make sure all tensors are on the correct device by explicitly using .to(device) when needed."]},{"cell_type":"markdown","metadata":{"id":"cZGOftdKy3cz"},"source":["### 2.a Vanilla RNN from scratch\n","\n","As a reminder, a vanilla RNN class's hidden state at timestep t is computed as:\n","$$\\begin{aligned}\n","h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n","\\end{aligned}\n","$$\n","\n","**Parameter initialization**:\n","\n","To initialize the parameters, use nn.Parameter(). We recommend using Glorot initialization to initialize the weights. You can also experiment with other weight initialization methods!\n","\n","**NOTE**: the convention we are following for this homework uses (num_hiddens, num_inputs) for the shape of $W_{xh}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tEXX-JjH05O"},"outputs":[],"source":["class RNNScratch(nn.Module):\n","    \"\"\"The RNN model implemented from scratch.\"\"\"\n","    def __init__(self, num_inputs, num_hiddens):\n","        super().__init__()\n","        self.num_hiddens = num_hiddens\n","        self.num_inputs = num_inputs\n","        ########################## YOUR CODE ##########################\n","        # Hint: use nn.Parameter()\n","        self.W_xh = None\n","        self.W_hh = None\n","        self.b_h = None\n","        ################################################################\n","\n","    def forward(self, inputs, state=None):\n","        outputs = []\n","        ########################## YOUR CODE ##########################\n","        # RNN updates the hidden state one timestep at a time.\n","        # Iterate over the first dimension of inputs (time steps),\n","        # then update the state iteratively\n","        # Make sure the initial hidden state has the correct shape: (batch_size, num_hiddens).\n","        # Shape of inputs: (num_steps, batch_size, num_inputs)\n","        ################################################################\n","        return outputs, state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuhGO6YIE5Ex"},"outputs":[],"source":["###################### SANITY CHECK ######################\n","def check_len(a, n):\n","    \"\"\"Check the length of a list.\"\"\"\n","    assert len(a) == n, f'list\\'s length {len(a)} != expected length {n}'\n","\n","def check_shape(a, shape):\n","    \"\"\"Check the shape of a tensor.\"\"\"\n","    assert a.shape == shape, \\\n","            f'tensor\\'s shape {a.shape} != expected shape {shape}'\n","\n","batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n","rnn = RNNScratch(num_inputs, num_hiddens).to(device)\n","X = torch.ones((num_steps, batch_size, num_inputs)).to(device)\n","outputs, state = rnn(X)\n","\n","check_len(outputs, num_steps)\n","check_shape(outputs[0], (batch_size, num_hiddens))\n","check_shape(state, (batch_size, num_hiddens))\n","###########################################################"]},{"cell_type":"markdown","source":["### 2.b GRU from scratch\n","\n","GRU (Gated Recurrent Unit) is a type of recurrent neural network designed to better handle long-term dependencies and reduce the vanishing gradient problem affecting vanilla RNNs. It does so by introducing update gate and reset gate, which control how much past information to keep and how much new information to add at each time step. A hidden state $h_t$ at timestep t is computed as:\n","\n","$$\n","\\begin{aligned}\n","z_t &= \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z) && \\text{(update gate)} \\\\\n","r_t &= \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r) && \\text{(reset gate)} \\\\\n","\\tilde{h}_t &= \\tanh(W_{xh} x_t + W_{hh} (r_t \\odot h_{t-1}) + b_h) && \\text{(candidate state)} \\\\\n","h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t && \\text{(new hidden state)}\n","\\end{aligned}\n","$$\n","\n","where $\\sigma$ is sigmoid activation, $\\odot$ an element-wise multiplication.\n","\n","At a high-level, the **update gate** ($z_t$) controls how much of the previous state to keep: $(1 - z_t) \\odot h_{t-1}$ retains part of the past memory, and $z_t \\odot \\tilde{h}_t$ adds new information from the new state. The **reset gate** ($r_t$) controls how much of the previous hidden state $h_{t-1}$ should influence the new candidate state $\\tilde{h}_t$. Notice that $r_{t}$ = 1 is equivalent to the hidden state update of vanilla RNN, whereas $r_t$ close to 0 down-weighs the influence of the previous hidden state, thereby \"resetting\" the memory.\n","\n","*To think about: Why is GRU better for handling long-term dependency? Why does it help with vanishing gradient problem?*\n","\n","**NOTE**: the convention we are following for this homework uses (num_hiddens, num_inputs) for the shape of $W_{xh}$, $W_{xr}$, $W_{xz}$."],"metadata":{"id":"kER5Pm1X5Ru-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IOUnjovy3c0"},"outputs":[],"source":["class GRUScratch(nn.Module):\n","    def __init__(self, num_inputs, num_hiddens):\n","        super().__init__()\n","\n","        self.num_hiddens = num_hiddens\n","        self.num_inputs = num_inputs\n","        ########################## YOUR CODE ##########################\n","        self.W_xz, self.W_hz, self.b_z = None, None, None  # Update gate\n","        self.W_xr, self.W_hr, self.b_r = None, None, None  # Reset gate\n","        self.W_xh, self.W_hh, self.b_h = None, None, None  # Candidate hidden state\n","        ################################################################\n","\n","    def forward(self, inputs, state=None):\n","        outputs=[]\n","        ########################## YOUR CODE ##########################\n","        ################################################################\n","        return outputs, state\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLT9yCJEy3c0"},"outputs":[],"source":["###################### SANITY CHECK ######################\n","batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n","gru = GRUScratch(num_inputs, num_hiddens).to(device)\n","X = torch.ones((num_steps, batch_size, num_inputs)).to(device)\n","outputs, state = gru(X)\n","\n","check_len(outputs, num_steps)\n","check_shape(outputs[0], (batch_size, num_hiddens))\n","check_shape(state, (batch_size, num_hiddens))\n","###########################################################"]},{"cell_type":"markdown","source":["### 2.c RNN LM wrapper\n","\n","Now we implement RNN LM wrapper which takes RNNScratch or GRUScratch class we built in the previous sections and adds embedding and output layers. This class handles input embedding (one_hot()), the end-to-end forward pass (forward()), and prediction given a prefix (predict()).\n","\n","NOTE:\n","* predict() should generates text tokens autoregressively starting from a given prefix. During the initial warm-up period, the model is fed the ground-truth prefix tokens instead of its own predictions, which helps the RNN build a hidden state representation of the prefix before it begins free generation."],"metadata":{"id":"neFw3EtdWuEz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGwqo1rSIH2a"},"outputs":[],"source":["class RNNLMScratch(nn.Module):\n","    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n","    def __init__(self, rnn, vocab_size, lr):\n","        super().__init__()\n","        self.rnn = rnn\n","        self.vocab_size = vocab_size\n","        self.lr = lr\n","        self.train_loss = []\n","        self.valid_loss = []\n","\n","        ########################## YOUR CODE #########################\n","        # TODO: initialize the embedding layer / output layer\n","        self.W_hq = None\n","        self.b_q = None\n","        ###############################################################\n","\n","    def one_hot(self, X):\n","        ########################## YOUR CODE #########################\n","        # Input shape : (batch_size, num_steps)\n","        # Output shape: (num_steps, batch_size, vocab_size)\n","        # NOTE: remember to set the torch dtype to torch.float32\n","        #       to maintain the same data type as the original X\n","        return X\n","        ###############################################################\n","\n","    def forward(self, X, state=None):\n","        ########################## YOUR CODE #########################\n","        # TODO: Apply the one-hot embedding,\n","        #       pass the embedding through RNN,\n","        #       return the rnn outputs and final rnn state after applying embedding and bias\n","        ###############################################################\n","        return None, None\n","\n","    def predict(self, prefix, num_preds, vocab, device=None):\n","\n","        state, outputs = None, [vocab[prefix[0]]]\n","        for t in range(len(prefix) + num_preds - 1):\n","            ########################## YOUR CODE #########################\n","            # TODO: Given the prefix, make num_preds many following predictions\n","            # Implement a separate logic for the initial warm-up period and\n","            # the actual model generation phase\n","            pass\n","            ###############################################################\n","        return ''.join([vocab.idx_to_token[i] for i in outputs])\n","\n","    def loss_fn(self, y_hat, Y):\n","        return F.cross_entropy(y_hat.reshape(-1, y_hat.shape[-1]), Y.reshape(-1))"]},{"cell_type":"markdown","metadata":{"id":"xrN7sxI3y3c1"},"source":["## 3. Simple trainer for model training (1.5 points)\n","\n","Next, you will implement a lightweight Trainer class to train the model. While popular libraries such as HuggingFace’s Trainer automate this process, you will build your own simplified version with core functionalities.\n","\n","TODO:\n","* Complete the **Trainer**: the trainer class should manage the training and validation loops, handle data loading, and track training and validation losses.\n","* Train your model until it achieves a validation loss below 1.8 on the dataset within 10 epochs.\n","\n","Note:\n","* `clip_gradients` is provided to keep the gradients from growing too large. Consider using this during your `Trainer.fit` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2_fEICHy3c1"},"outputs":[],"source":["class Trainer:\n","    def __init__(self, max_epochs, batch_size, device, gradient_clip_val=1):\n","        self.max_epochs = max_epochs\n","        self.batch_size = batch_size\n","        self.gradient_clip_val = gradient_clip_val\n","        self.device = device\n","        self.train_loss = [] # record the avg. batch loss every epoch\n","        self.valid_loss = [] # record the avg. batch loss every epoch\n","\n","    @staticmethod\n","    def clip_gradients(model, max_norm):\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","\n","    def get_dataloader(self, data):\n","        train_size = int(0.8 * len(data))\n","        train_data, val_data = random_split(data, [train_size, len(data) - train_size])\n","        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n","        valid_loader = DataLoader(val_data, batch_size=self.batch_size)\n","\n","        return train_loader, valid_loader\n","\n","    def fit(self, model, data, optimizer=None):\n","        model.to(self.device)\n","        if optimizer is None:\n","            optimizer = torch.optim.SGD(model.parameters(), lr=model.lr)\n","        train_loader, valid_loader = self.get_dataloader(data)\n","\n","        for epoch in range(self.max_epochs):\n","            model.train()\n","            train_loss = 0\n","            valid_loss = 0\n","            ########################### YOUR CODE ###################################\n","            # TODO: Train the model for max_epochs many steps\n","            # Complete a single forward and backward pass on a given training batch\n","            # Record the training loss\n","            ########################################################################\n","            self.train_loss.append(train_loss / len(train_loader))\n","\n","            model.eval()\n","            with torch.no_grad():\n","                ########################### YOUR CODE ###################################\n","                # TODO: at the end of each epoch, evaluate the model on the validation set.\n","                # Complete a single forward pass on a given validation batch\n","                # Record the validation loss\n","                pass\n","                ########################################################################\n","            self.valid_loss.append(valid_loss / len(valid_loader))\n","\n","            print(f\"Epoch {epoch+1} train loss: {self.train_loss[-1]}, validation loss {self.valid_loss[-1]}\")"]},{"cell_type":"markdown","metadata":{"id":"R8QvThqLy3c1"},"source":["### Verify your results!\n","\n","Note: this won't run if part 1, part 2 are not completed."]},{"cell_type":"code","source":["################# DO NOT CHANGE #################\n","def train_rnnlm(data, rnn_class, num_hiddens, lr, batch_size, num_epochs=10):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    rnn = rnn_class(num_inputs=len(data.vocab), num_hiddens=num_hiddens).to(device)\n","    model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=lr).to(device)\n","    trainer = Trainer(batch_size=batch_size, max_epochs=num_epochs, gradient_clip_val=1, device=device)\n","    trainer.fit(model, data)\n","    return trainer, model"],"metadata":{"id":"JfY8eF3NfYJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################ TO SUBMIT ################\n","# Feel free to tweak the hyperparameters to achieve a better loss!\n","RNN_HYPERPARAM = {'num_hiddens': 64, 'lr': 2, 'batch_size': 128}\n","GRU_HYPERPARAM = {'num_hiddens': 64, 'lr': 3, 'batch_size': 128}"],"metadata":{"id":"22ztM6TpfD1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICrfhRyXy3c1"},"outputs":[],"source":["################# CHECK YOUR RESULT! #################\n","if __name__ == \"__main__\":\n","    seed=123\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    random.seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    shakes_data = CharDataset(num_steps=32, raw_text=shakes_ds[:50000])\n","    rnn_trainer, rnn_model = train_rnnlm(data=shakes_data,\n","                                         rnn_class=RNNScratch,\n","                                         num_hiddens=RNN_HYPERPARAM['num_hiddens'],\n","                                         lr=RNN_HYPERPARAM['lr'],\n","                                         batch_size=RNN_HYPERPARAM['batch_size'],\n","                                         num_epochs=10\n","                                         )\n","    print('\\n')\n","    gru_trainer, gru_model = train_rnnlm(data=shakes_data,\n","                                         rnn_class=GRUScratch,\n","                                         num_hiddens=GRU_HYPERPARAM['num_hiddens'],\n","                                         lr=GRU_HYPERPARAM['lr'],\n","                                         batch_size=GRU_HYPERPARAM['batch_size'],\n","                                         num_epochs=10\n","                                         )\n","\n","    inputs = ['the mind in',\n","              'he saw a fox ',\n","              'thou art ',\n","              'to be or not ',\n","              'she stood on ',\n","              'before the start ',\n","              'thou speakest']\n","\n","    print(\"\\n\")\n","    print(\"RNN predictions:\")\n","    for i in range(len(inputs)):\n","        print(rnn_model.predict(inputs[i], 30, shakes_data.vocab, device=device))\n","\n","    print('\\n')\n","    print(\"GRU predictions:\")\n","    for i in range(len(inputs)):\n","        print(gru_model.predict(inputs[i], 30, shakes_data.vocab, device=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNoKyBhUy3c2"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    f, ax = plt.subplots(ncols=2, figsize=(8,4))\n","    ax[0].plot(rnn_trainer.train_loss, label='rnn')\n","    ax[0].plot(gru_trainer.train_loss, label='gru')\n","    ax[0].set_title(\"Train loss\")\n","    ax[1].plot(rnn_trainer.valid_loss, label='rnn')\n","    ax[1].plot(gru_trainer.valid_loss, label='gru')\n","    ax[1].set_title(\"Validation loss\")\n","    ax[0].legend()\n","    ax[1].legend()"]},{"cell_type":"markdown","source":["# Collaboration / External Help\n","Disclose any help you used (LLM usage, blogs, search, Github links, etc) and collaborations with your classmates. If you  completed the homework on your own, you can leave this part empty.\n","\n","> TODO"],"metadata":{"id":"LjHT-qthu4as"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1HWnL_gbkF03xfvzU2PkJpiqo0FZdg0J9","timestamp":1761950869427}]},"gpuClass":"standard","kernelspec":{"display_name":"dataid_env2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}